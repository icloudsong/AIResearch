# 基于神经网络的混合计算(DNC)-Hybrid computing using a NN with dynamic external memory

## 1. 摘要
ANN非常擅长感知处理、 序列学习、 增强学习，而由于外部存储器的缺失，在表达变量、数据结构和存储长时间数据上能力有限。
在此我们介绍一种机器学习模型称为可微神经计算机 (DNC) ，包含一个可以读取和写入外部存储器的神经网络，类似于传统计算机的随机存储器。正如传统计算机，可以用内存来表达和操纵复杂的数据结构，并且，类似于一个神经网络，依然可以从数据中进行学习。
当使用监督学习进行训练时，我们可以确定，DNC 可以成功地解答用来模仿自然语言中的推理和判断的综合问题。我们可以得到，它可以进行任务学习，例如查找随机图中指定点之间的最短路径和推断的缺失环节，之后再将这种能力泛化，用于交通线路图、家谱等特定的图。
使用强化学习训练后，DNC 能够完成移动拼图这个益智游戏，其中游戏目标可以使用序列符号进行表示。
综上所述，我们的成果展示了 DNC 拥有解决复杂、结构化任务的能力，这些任务是没有外部可读写的存储器的神经网络难以胜任的。

## 2. 前言
现代计算机普遍使用计算和数据分离的计算体系，计算和输入输出分离。这包含两个便利：分层的存储结构带来价格和存储的折中。但是变量的读取和生成需要运算器对地址进行操作，不好之处就是，在内存动态增长的网络中，网络不能进行随机动态进行存储操作。
最近的在信号处理、序列学习、强化学习、认知科学和神经科学有很大突破，但在表达变量和数据结构时受到限制。此文旨在通过提供一个结合神经网络和外部存储器的结构，结合神经网络和计算处理的优势，方法是聚焦于最小化备忘录memoranda/内存和长时间存储器的接口。整个系统是可微的，因此可以使用随机梯度下降法进行端到端的训练，允许网络学习如何 在有目的行为中操作和组织内存

## 系统概览
DNC 是一种耦合到外部存储矩阵的神经网络（只要内存不被占用完全，网络的行为与内存块的大小独立|应该是使用了分布表进行去位置相关|，因此我们认为内存是“外部的”）。如果内存可以被认为是 DNC 的 RAM，网络则可以被称为控制器，CPU可微的操作是通过梯度下降法直接进行学习。DNC的早期结构，神经图灵机，拥有相似的结构，但使用了更受限的内存存取方法。
DNC 架构不同于最近提出的Memory networks和Pointer networks的神经记忆框架，其区别在于DNC内存有选择性地可以写入和读取，允许迭代修改内存内容。
相比传统计算机使用唯一编址内存，DNC使用可微注意/分析机制[2,16-18]定义指派内存第N行或者“位置”，在N*W的矩阵M中（这样直接定义内存有问题啊），这些分派，这里我们成为权值，表示此处位置涉及到读或者写的程度/度量？。读向量r通过对记忆矩阵M的一个读权值操作wr返回( 记忆位置的权值累加和 )：

类比，写操作符使用一个写权值wW首先擦除向量e，然后加和一个向量v：
                        M[ i, j ] <—— M[ i, j ]
决定 和应用权值的单元叫做读写头。头的操作可由表1进行阐述。
![image](https://user-images.githubusercontent.com/17287752/141486572-27a40c68-7a75-42fa-90da-8c117f371a89.png)


## 结构

a,一个DNC结构从额外的数据源接受数据输入并产生输出;

b,c,控制器可以写/输出向量（参数化一个写磁头-绿色）且并联一个读磁头（上图中有两个，蓝色和粉色）。

写磁头定义了一个写和擦除向量（用于编辑N*M内存块），其元素的量级和符号通过块区域和shading唯一表示。另外，一个写键用来查找内容去寻找先前写过的位置（待编辑）。写键可以用于定义一个权值（有选择的）确定于写操作在矩阵块的行或者位置。

读磁头可以使用门（被称作读模式）来进行 使用一个读键（“C”）进行内容查找，和读出位置后（使用F键进行前向搜索或者“B”键进行后项）写入。

d.使用标记位置向量 记录目前已使用位置，一个缓存链接矩阵记录被写入的顺序；图中，我们使用有向箭头表示写入的顺序。
