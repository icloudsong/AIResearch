# 从0开始词嵌入（Word embedding）
## 1. 什么是词嵌入？
词嵌入（Word embedding）是NLP（Natural Language Processing 自然语言处理）中语言模型与表征学习技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间嵌入到一个维数低得多的连续向量空间
中，每个单词或词组被映射为实数域上的向量[1]。
众所周知，计算机无法读懂自然语言，只能处理数值，因此自然语言需要以一定的形式转化为数值。词嵌入就是将自然语言中的词语映射为数值的一种方式。然而对于丰富的自然语言来说，将它们映射为数值向量，使之包含
更丰富的语义信息和抽象特征显然是一种更好的选择。词嵌入是NLP领域中下游任务实现的重要基础，目前的大多数NLP任务都离不开词嵌入。并且纵观NLP的发展史，很多革命性的成果也都是词嵌入的发展成果，如Word2Vec、GloVe、FastText、ELMo和BERT[2]。
本章将对现有的多种词嵌入方法进行梳理和比对，以帮助读者更好地理解词嵌入的核心思想。

## 2. 词嵌入方法
### 2.1. One-Hot（独热编码）模型
在最初NLP任务中，非结构化的文本数据转换成可供计算机识别的数据形式使用的是独热编码模型(one-hot code)，它将文本转化为向量形式表示，并且不局限于语言种类，也是最简单的词嵌入的方式。
One-Hot将词典中所有的词排成一列，根据词的位置设计向量，例如词典中有m个词,则每个单词都表示为一个m维的向量，单词对应词典中的位置的维度上为1，其他维度为0。
如图2.1 所示，将一个句子中的每个“字”作为“词”表示，将得到每个“词”的向量表示，由于一共有5种词，所以将它们映射为5维的向量。
### 2.1.1. 实验
from sklearn.feature_extraction.text import CountVectorizer
​
def one_hot(texts):
    '''
    CountVectorizer：文本特征提取计算类，会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数
    '''
    vectorizer = CountVectorizer(analyzer="char",binary=True)
    texts = vectorizer.fit_transform(texts)  # 拟合模型，并返回文本矩阵
    return texts
​
​
text = ['东', '北', '大', '学', '在', '东', '北']
text = one_hot(text)  # 此处text为csr_matrix类型，是一个稀疏矩阵。如：(2, 3)    1代表第二行第三列的值为1，其余全为0。
text
# 输出
  (0, 0)    1   # [1,0,0,0,0] 东
  (1, 1)    1   # [0,1,0,0,0] 北
  (2, 3)    1   # [0,0,0,1,0] 大
  (3, 4)    1   # [0,0,0,0,1] 学
  (4, 2)    1   # [0,0,1,0,0] 在
  (5, 0)    1   # [1,0,0,0,0] 东
  (6, 1)    1   # [0,1,0,0,0] 北
